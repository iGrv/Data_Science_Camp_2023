{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home task: Sentiment Analysis\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load data \n",
    "\n",
    "</font>\n",
    "\n",
    "[Sentiment Analysis Dataset](https://www.kaggle.com/sonaam1234/sentimentdata)\n",
    "\n",
    "alternative source:\n",
    "[rt-polaritydata](https://github.com/dennybritz/cnn-text-classification-tf/tree/master/data/rt-polaritydata)\n",
    "\n",
    "alternative source:\n",
    "[Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data)\n",
    "\n",
    "Each line in these two files corresponds to a single snippet (usually containing roughly one single sentence); all snippets are down-cased.  \n",
    "[More info about dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CWD = os.getcwd()  # Current working directory\n",
    "DATA_DIR = os.path.join(CWD, 'data')\n",
    "NEG_FILE = os.path.join(DATA_DIR, 'rt-polarity.neg')\n",
    "POS_FILE = os.path.join(DATA_DIR, 'rt-polarity.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of texts_neg = 5,331\n",
      "\n",
      "1: simplistic , silly and tedious . \n",
      "2: it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "3: exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "4: [garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "5: a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n"
     ]
    }
   ],
   "source": [
    "with open(NEG_FILE, 'r', encoding='utf-8', errors='ignore') as f:  # Some invalid symbols encountered \n",
    "    content = f.read()  \n",
    "texts_neg = content.splitlines()\n",
    "\n",
    "print('Length of texts_neg = {:,}'.format(len(texts_neg)), end='\\n\\n')\n",
    "for i, review in enumerate(texts_neg[:5]):\n",
    "    print(f'{i + 1}:', review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of texts_pos = 5,331\n",
      "\n",
      "1: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "2: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "3: effective but too-tepid biopic\n",
      "4: if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "5: emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n"
     ]
    }
   ],
   "source": [
    "with open(POS_FILE, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "texts_pos = content.splitlines()\n",
    "\n",
    "print('Length of texts_pos = {:,}'.format(len(texts_pos)), end='\\n\\n')\n",
    "for i, review in enumerate(texts_pos[:5]):\n",
    "    print(f'{i + 1}:', review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    Performs preprocessing of `text` and return a list of its words\n",
    "    containing at least one alphabetic character and being not stop words\n",
    "    :param text: text to preprocess\n",
    "    :type text: str\n",
    "    :return: a list of words from `text`\n",
    "    :rtype: list[str]\n",
    "    '''\n",
    "    return [\n",
    "        word for word in word_tokenize(text)\n",
    "        if any(c.isalpha() for c in word) and word not in STOPWORDS\n",
    "    ]\n",
    "\n",
    "# Prepocess negative and positive reviews to get all tokens\n",
    "all_tokens = []\n",
    "for text in (texts_neg + texts_pos):\n",
    "    all_tokens.extend(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common words in reviews: (\"'s\", 'film', 'movie', \"n't\", 'one', 'like', 'story', 'much', 'even', 'good')\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# Create a list containing tokens sorted by their frequency in reviews\n",
    "vocab = FreqDist(all_tokens)\n",
    "most_common_words, _ = zip(*vocab.most_common())\n",
    "print(f'Top 10 most common words in reviews: {most_common_words[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(text, n):\n",
    "    '''\n",
    "    Converts a sentence into features accepted by the NLTK classifier\n",
    "    :param text: a sentence\n",
    "    :type text: str\n",
    "    :param n: number of most common words from `most_common_words` to use\n",
    "    :type n: int\n",
    "    :return: a dictionary mapping words from `most_common_words` to bool values:\n",
    "    True if a word containg in `text`, False otherwise (so called featureset)\n",
    "    :rtype: dict[str, bool]\n",
    "    '''\n",
    "    tokens = set(preprocess(text))  # Preprocess the text to get its words (tokens)\n",
    "    return {word: word in tokens for word in most_common_words[:n]}\n",
    "\n",
    "# Construct data from negative and positive reviews as featuresets\n",
    "text_data = [\n",
    "    (word_features(text, 2000), 'neg') for text in texts_neg\n",
    "] + [\n",
    "    (word_features(text, 2000), 'pos') for text in texts_pos\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 10662\n",
      "Number of training samples: 8529\n",
      "Number of test samples: 2133\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Split text_data into training and test sets (80% for training set, 20% for test set)\n",
    "random.shuffle(text_data)\n",
    "threshold = int(0.8 * len(text_data))\n",
    "text_train, text_test = text_data[:threshold], text_data[threshold:]\n",
    "\n",
    "print('Total number of samples:', len(text_data))\n",
    "print('Number of training samples:', len(text_train))\n",
    "print('Number of test samples:', len(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "accuracy: 0.7431\n",
      "\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     19.7 : 1.0\n",
      "                    warm = True              pos : neg    =     19.7 : 1.0\n",
      "                 generic = True              neg : pos    =     15.6 : 1.0\n",
      "             examination = True              pos : neg    =     14.4 : 1.0\n",
      "                 routine = True              neg : pos    =     14.3 : 1.0\n",
      "               inventive = True              pos : neg    =     13.7 : 1.0\n",
      "                  boring = True              neg : pos    =     13.0 : 1.0\n",
      "                    flat = True              neg : pos    =     12.6 : 1.0\n",
      "              refreshing = True              pos : neg    =     11.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "\n",
    "# Use Naive Bayes to perform review classification \n",
    "clf_nb = NaiveBayesClassifier.train(text_train)\n",
    "\n",
    "print('Naive Bayes:')\n",
    "print(f'accuracy: {nltk_accuracy(clf_nb, text_test):.4f}', end='\\n\\n')\n",
    "\n",
    "# Find top 10 most informative features (words) from most_common_words\n",
    "clf_nb.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[garbus] discards the potential for pathologic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>both exuberantly romantic and serenely melanch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>mazel tov to a film about a family's joyous li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>standing in the shadows of motown is the best ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>it's nice to see piscopo again after all these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>provides a porthole into that noble , tremblin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rating\n",
       "0                      simplistic , silly and tedious .        0\n",
       "1      it's so laddish and juvenile , only teenage bo...       0\n",
       "2      exploitative and largely devoid of the depth o...       0\n",
       "3      [garbus] discards the potential for pathologic...       0\n",
       "4      a visually flashy but narratively opaque and e...       0\n",
       "...                                                  ...     ...\n",
       "10657  both exuberantly romantic and serenely melanch...       1\n",
       "10658  mazel tov to a film about a family's joyous li...       1\n",
       "10659  standing in the shadows of motown is the best ...       1\n",
       "10660  it's nice to see piscopo again after all these...       1\n",
       "10661  provides a porthole into that noble , tremblin...       1\n",
       "\n",
       "[10662 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame containing reviews and their ratings (0 for negative, 1 for positive) to use with scikit-learn classifiers\n",
    "texts_df = pd.DataFrame({'review': (texts_neg + texts_pos), 'rating': ([0]*len(texts_neg) + [1]*len(texts_pos))})\n",
    "texts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split texts_df into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts_df['review'], texts_df['rating'], train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Transform texts into a matrix of unigram and bigram counts using a count vectorizer\n",
    "vectorizer = CountVectorizer(max_features=50000, ngram_range=(1, 2))\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "accuracy: 0.7764\n",
      "F1 score: 0.7827\n",
      "AUC: 0.8566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Use logistic regression for classification \n",
    "clf_logreg = LogisticRegression(max_iter=3000, random_state=0).fit(X_train_vectorized, y_train)\n",
    "preds = clf_logreg.predict(X_test_vectorized)\n",
    "scores = clf_logreg.decision_function(X_test_vectorized)\n",
    "\n",
    "# Evaluate the logistic regression model\n",
    "print('Logistic Regression:')\n",
    "print(f'accuracy: {accuracy_score(y_test, preds):.4f}')\n",
    "print(f'F1 score: {f1_score(y_test, preds):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_test, scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n-grams which have highest among the -coef values (tend to indicate a negative rating):\n",
      "['neither' 'pretentious' 'unfunny' 'bore' 'worst' 'badly' 'too' 'boring'\n",
      " 'bad' 'dull']\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "coef = clf_logreg.coef_.squeeze()\n",
    "\n",
    "# Find top 10 n-grams indicating a negative review using -coef of the model\n",
    "coef_neg_index = (-coef).argsort()\n",
    "print('Top 10 n-grams which have highest among the -coef values (tend to indicate a negative rating):')\n",
    "print(features[coef_neg_index[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n-grams which have highest among the +coef values (tend to indicate a positive rating):\n",
      "['remarkable' 'unexpected' 'engrossing' 'solid' 'works' 'fun' 'hilarious'\n",
      " 'wonderful' 'enjoyable' 'powerful']\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 n-grams indicating a positive review using coef of the model\n",
    "coef_pos_index = coef.argsort()\n",
    "print('Top 10 n-grams which have highest among the +coef values (tend to indicate a positive rating):')\n",
    "print(features[coef_pos_index[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machines:\n",
      "accuracy: 0.7632\n",
      "F1 score: 0.7701\n",
      "AUC: 0.8455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Use support vector machines (linear) for classification\n",
    "clf_svm = LinearSVC(max_iter=3000, random_state=0).fit(X_train_vectorized, y_train)\n",
    "preds = clf_svm.predict(X_test_vectorized)\n",
    "scores = clf_svm.decision_function(X_test_vectorized)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "print('Linear Support Vector Machines:')\n",
    "print(f'accuracy: {accuracy_score(y_test, preds):.4f}')\n",
    "print(f'F1 score: {f1_score(y_test, preds):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_test, scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Transform texts into a matrix of unigrams' and bigrams' TF-IDF values using a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n-grams which have lowest TF-IDF values: ['unnamed easily' 'they exist' 'well they' 're coming' 'they they'\n",
      " 'unnamed' 'they well' 're they' 're back' 'whatever terror']\n",
      "\n",
      "Top 10 n-grams which have highest TF-IDF values: ['crummy' 'pathetic' 'fantastic' 'bland' 'fun' 'terrible' 'indeed'\n",
      " 'refreshing' 'slummer' 'and']\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Find top 10 n-grams with lowest TF-IDF values and top 10 n-grams with highest TF-IDF values\n",
    "tfidf_index = X_train_vectorized.max(axis=0).toarray().squeeze().argsort()\n",
    "print(f'Top 10 n-grams which have lowest TF-IDF values: {features[tfidf_index[:10]]}', end='\\n\\n')\n",
    "print(f'Top 10 n-grams which have highest TF-IDF values: {features[tfidf_index[-10:]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "accuracy: 0.7689\n",
      "F1 score: 0.7774\n",
      "AUC: 0.8511\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression for classification\n",
    "clf_logreg = LogisticRegression(max_iter=3000, random_state=0).fit(X_train_vectorized, y_train)\n",
    "preds = clf_logreg.predict(X_test_vectorized)\n",
    "scores = clf_logreg.decision_function(X_test_vectorized)\n",
    "\n",
    "# Evaluate the logistic regression model\n",
    "print('Logistic Regression:')\n",
    "print(f'accuracy: {accuracy_score(y_test, preds):.4f}')\n",
    "print(f'F1 score: {f1_score(y_test, preds):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_test, scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machines:\n",
      "accuracy: 0.7886\n",
      "F1 score: 0.7951\n",
      "AUC: 0.8686\n"
     ]
    }
   ],
   "source": [
    "# Use support vector machines (linear) for classification\n",
    "clf_svm = LinearSVC(max_iter=3000, random_state=0).fit(X_train_vectorized, y_train)\n",
    "preds = clf_svm.predict(X_test_vectorized)\n",
    "scores = clf_svm.decision_function(X_test_vectorized)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "print('Linear Support Vector Machines:')\n",
    "print(f'accuracy: {accuracy_score(y_test, preds):.4f}')\n",
    "print(f'F1 score: {f1_score(y_test, preds):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_test, scores):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
