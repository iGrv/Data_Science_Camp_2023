{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green>\n",
    "\n",
    "#  Home task: Support Vector Machine\n",
    "</font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()                 # Current working directory\n",
    "path = os.path.join(cwd, 'data')  # Data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font color = green>\n",
    "\n",
    "##  Sklearn SVM for Iris dataset\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Normalize feature values\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM classifier (linear kernel):\n",
      "train accuracy = 96.429%\n",
      "test accuracy = 97.368%\n",
      "\n",
      "SVM classifier (polynomial kernel, degree = 3):\n",
      "train accuracy = 86.607%\n",
      "test accuracy = 84.211%\n",
      "\n",
      "SVM classifier (RBF kernel, gamma = 0.5):\n",
      "train accuracy = 93.750%\n",
      "test accuracy = 94.737%\n"
     ]
    }
   ],
   "source": [
    "# YOUR_CODE.  Try linear, rbf and poly kernels \n",
    "# START_CODE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Use SVM classifier with linear kernel\n",
    "clf_svm = SVC(C=0.1, kernel='linear').fit(X_train_scaled, y_train)\n",
    "print('SVM classifier (linear kernel):')\n",
    "print('train accuracy = {:.3%}'.format(clf_svm.score(X_train_scaled, y_train)))\n",
    "print('test accuracy = {:.3%}'.format(clf_svm.score(X_test_scaled, y_test)), end='\\n\\n')\n",
    "\n",
    "# Use SVM classifier with polynomial kernel\n",
    "clf_svm = SVC(C=0.1, kernel='poly', degree=3).fit(X_train_scaled, y_train)\n",
    "print('SVM classifier (polynomial kernel, degree = 3):')\n",
    "print('train accuracy = {:.3%}'.format(clf_svm.score(X_train_scaled, y_train)))\n",
    "print('test accuracy = {:.3%}'.format(clf_svm.score(X_test_scaled, y_test)), end='\\n\\n')\n",
    "\n",
    "# Use SVM classifier with RBF kernel\n",
    "clf_svm = SVC(C=0.1, kernel='rbf', gamma=0.5).fit(X_train_scaled, y_train)\n",
    "print('SVM classifier (RBF kernel, gamma = 0.5):')\n",
    "print('train accuracy = {:.3%}'.format(clf_svm.score(X_train_scaled, y_train)))\n",
    "print('test accuracy = {:.3%}'.format(clf_svm.score(X_test_scaled, y_test)))\n",
    "# END_CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font color = green>\n",
    "\n",
    "##  Spam/Non-spam classification\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green>\n",
    "\n",
    "###  Problem statement\n",
    "</font>\n",
    "\n",
    "Spam filter classifies emails into spam and non-spam email. \n",
    "For this task use SVMs to build your spam filter.\n",
    "This is binary classification  - whether a given email, x, is spam (y = 1) or non-spam (y = 0).\n",
    "In particular, you need to convert each email into a feature vector x. \n",
    "The dataset for this task is based on a a subset of the SpamAssassin Public Corpus.\n",
    "For the purpose of this exercise, it will use the body of the email (excluding the email headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green>\n",
    "\n",
    "###  Preprocessing mail\n",
    "</font>\n",
    "\n",
    "Before starting on a machine learning task, it is usually insightful to take a look at examples from the dataset. Below is a sample email that contains a URL, an email address (at the end), numbers, and dollar amounts. While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every email. Therefore, one method often employed in processing emails is to “normalize” these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, replace each URL in the email with the unique string “http_addr” to indicate that a URL was present.\n",
    "This lets the spam classifier to make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green>\n",
    "\n",
    "### Reveiw sample mail \n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Anyone knows how much it costs to host a web portal ?\n",
      ">\n",
      "Well, it depends on how many visitors you're expecting.\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
      "if youre running something big..\n",
      "\n",
      "To unsubscribe yourself from this mailing list, send an email to:\n",
      "groupname-unsubscribe@egroups.com\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sample(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "    \n",
    "file = os.path.join(path , 'emailSample1.txt')\n",
    "content = get_sample(file)\n",
    "print(content)\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Preprocessing and normalization steps\n",
    "</font>\n",
    "\n",
    "Preprocessing and normalization includes the following steps:\n",
    "<ul>\n",
    "<li> <b>Lower-casing</b>: The entire email is converted into lower case, so that capitalization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "    \n",
    "<li> <b>Stripping HTML</b>: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "<li> <b>Normalizing URLs</b>: All URLs are replaced with the text “httpaddr”.\n",
    "<li> <b>Normalizing Email Addresses</b>: All email addresses are replaced\n",
    "with the text “emailaddr”.\n",
    "<li> <b>Normalizing Numbers</b>: All numbers are replaced with the text\n",
    "“number”.\n",
    "<li> <b>Normalizing Dollars</b>: All dollar signs ($) are replaced with the text\n",
    "“dollar”.\n",
    "<li> <b>Word Stemming</b>: Words are reduced to their stemmed form. For example, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Sometimes, the Stemmer actually strips on additional characters from the end, so “include”, “includes”, “included”, and “including” are all replaced with “includ”.\n",
    "<li> <b>Removal of non-words</b>: Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Word tokenization\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(content):\n",
    "    '''\n",
    "    content: str - body of mail\n",
    "    return: list of tokens (str) e.g. ['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a']\n",
    "    '''\n",
    "    # YOUR_CODE.  Split the content to tokens. You may need re.split()\n",
    "    # START_CODE\n",
    "    tokens = re.split('\\s', content)\n",
    "    # END_CODE\n",
    "    \n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
       "       'a', 'web', 'portal', '?', '>', 'Well,', 'it', 'depends', 'on',\n",
       "       'how', 'many', 'visitors', \"you're\", 'expecting.', 'This', 'can',\n",
       "       'be', 'anywhere', 'from', 'less', 'than', '10', 'bucks', 'a',\n",
       "       'month', 'to', 'a', 'couple', 'of', '$100.', '', 'You', 'should',\n",
       "       'checkout', 'http://www.rackspace.com/', 'or', 'perhaps', 'Amazon',\n",
       "       'EC2', '', 'if', 'youre', 'running', 'something', 'big..', '',\n",
       "       'To', 'unsubscribe', 'yourself', 'from', 'this', 'mailing',\n",
       "       'list,', 'send', 'an', 'email', 'to:',\n",
       "       'groupname-unsubscribe@egroups.com', '', ''], dtype='<U33')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize('''> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n''')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`array(['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
    "       'a', 'web', 'portal', '?', '>', 'Well', 'it', 'depends', 'on',\n",
    "       'how', 'many', 'visitors', \"you're\", 'expecting.', 'This', 'can',\n",
    "       'be', 'anywhere', 'from', 'less', 'than', '10', 'bucks', 'a',\n",
    "       'month', 'to', 'a', 'couple', 'of', '$100.', '', 'You', 'should',\n",
    "       'checkout', 'http://www.rackspace.com/', 'or', 'perhaps', 'Amazon',\n",
    "       'EC2', '', 'if', 'youre', 'running', 'something', 'big..', '',\n",
    "       'To', 'unsubscribe', 'yourself', 'from', 'this', 'mailing', 'list',\n",
    "       'send', 'an', 'email', 'to:', 'groupname-unsubscribe@egroups.com',\n",
    "       '', ''], dtype='<U33')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Lower case \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(tokens):\n",
    "    '''\n",
    "    tokens: ndarray of str\n",
    "    return: ndarray of tokens in lower case (str)\n",
    "    '''\n",
    "    # YOUR_CODE.  Make all tokens in lower case\n",
    "    # START_CODE\n",
    "    tokens = np.vectorize(str.lower)(tokens)\n",
    "    # END_CODE\n",
    "   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['>', 'anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
       "       'a', 'web', 'portal', '?', '>', 'well,', 'it', 'depends', 'on',\n",
       "       'how', 'many', 'visitors', \"you're\", 'expecting.', 'this', 'can',\n",
       "       'be', 'anywhere', 'from', 'less', 'than', '10', 'bucks', 'a',\n",
       "       'month', 'to', 'a', 'couple', 'of', '$100.', '', 'you', 'should',\n",
       "       'checkout', 'http://www.rackspace.com/', 'or', 'perhaps', 'amazon',\n",
       "       'ec2', '', 'if', 'youre', 'running', 'something', 'big..', '',\n",
       "       'to', 'unsubscribe', 'yourself', 'from', 'this', 'mailing',\n",
       "       'list,', 'send', 'an', 'email', 'to:',\n",
       "       'groupname-unsubscribe@egroups.com', '', ''], dtype='<U33')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = lower_case(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`array(['>', 'anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
    "       'a', 'web', 'portal', '?', '>', 'well', 'it', 'depends', 'on',\n",
    "       'how', 'many', 'visitors', \"you're\", 'expecting.', 'this', 'can',\n",
    "       'be', 'anywhere', 'from', 'less', 'than', '10', 'bucks', 'a',\n",
    "       'month', 'to', 'a', 'couple', 'of', '$100.', '', 'you', 'should',\n",
    "       'checkout', 'http://www.rackspace.com/', 'or', 'perhaps', 'amazon',\n",
    "       'ec2', '', 'if', 'youre', 'running', 'something', 'big..', '',\n",
    "       'to', 'unsubscribe', 'yourself', 'from', 'this', 'mailing', 'list',\n",
    "       'send', 'an', 'email', 'to:', 'groupname-unsubscribe@egroups.com',\n",
    "       '', ''], dtype='<U33')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Normalize\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(tokens):\n",
    "    '''\n",
    "    tokens: ndarray of str\n",
    "    return: ndarray of tokens replaced with corresponding unified words\n",
    "    '''\n",
    "    # YOUR_CODE.  \n",
    "        # remove html and other tags\n",
    "        # mark all numbers as 'number'\n",
    "        # mark all urls as 'httpaddr'\n",
    "        # mark all emails as 'emailaddr'\n",
    "        # replace $ as 'dollar'\n",
    "        # get rid of any punctuation\n",
    "        # remove any non alphanumeric characters\n",
    "    #  You may  need re.sub()\n",
    "    # START_CODE\n",
    "\n",
    "    # Replace urls with 'httpaddr'\n",
    "    tokens = np.vectorize(lambda x: re.sub(r'https?://(www.)?[\\w\\d!\"#$%&\\'()*+-,./:;<=>?@\\^_`{|}~-]+', 'httpaddr', x))(tokens)\n",
    "\n",
    "    # Replace emails with 'emailaddr'\n",
    "    email_pattern = (\n",
    "        r'[\\w\\d!\"#$%&\\'()*+-,/:;<=>?\\^_`{|}~-]+\\.*[\\w\\d!\"#$%&\\'()*+-,/:;<=>?\\^_`{|}~-]+'  # Local part\n",
    "        r'@'                                                                              # at sign\n",
    "        r'[\\w\\d!\"#$%&\\'()*+,./:;<=>?\\^_`{|}~-]+\\-*[\\w\\d!\"#$%&\\'()*+,./:;<=>?\\^_`{|}~-]+'  # Domain part\n",
    "    )\n",
    "    tokens = np.vectorize(lambda x: re.sub(email_pattern, 'emailaddr', x))(tokens)\n",
    "\n",
    "    # Replace dollar sign '$' with 'dollar'\n",
    "    tokens = np.vectorize(lambda x: re.sub(r'\\$', 'dollar', x))(tokens)\n",
    "\n",
    "    # Remove html tags, punctuation and non alphanumeric characters\n",
    "    tokens = np.vectorize(lambda x: re.sub(r'(<.+?>)|[!\"#$%&\\'()*+-,./:;<=>?@\\^_`{|}~-]|\\W', '', x))(tokens)\n",
    "\n",
    "    # Replace numbers with 'number'\n",
    "    tokens = np.vectorize(lambda x: re.sub(r'\\d+', 'number', x))(tokens)\n",
    "    # END_CODE\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
       "       'a', 'web', 'portal', '', '', 'well', 'it', 'depends', 'on', 'how',\n",
       "       'many', 'visitors', 'youre', 'expecting', 'this', 'can', 'be',\n",
       "       'anywhere', 'from', 'less', 'than', 'number', 'bucks', 'a',\n",
       "       'month', 'to', 'a', 'couple', 'of', 'dollarnumber', '', 'you',\n",
       "       'should', 'checkout', 'httpaddr', 'or', 'perhaps', 'amazon',\n",
       "       'ecnumber', '', 'if', 'youre', 'running', 'something', 'big', '',\n",
       "       'to', 'unsubscribe', 'yourself', 'from', 'this', 'mailing', 'list',\n",
       "       'send', 'an', 'email', 'to', 'emailaddr', '', ''], dtype='<U12')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = normalize_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`array(['', 'anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
    "       'a', 'web', 'portal', '', '', 'well', 'it', 'depends', 'on', 'how',\n",
    "       'many', 'visitors', 'youre', 'expecting', 'this', 'can', 'be',\n",
    "       'anywhere', 'from', 'less', 'than', 'number', 'bucks', 'a',\n",
    "       'month', 'to', 'a', 'couple', 'of', 'dollarnumber', '', 'you',\n",
    "       'should', 'checkout', 'httpaddr', 'or', 'perhaps', 'amazon',\n",
    "       'ecnumber', '', 'if', 'youre', 'running', 'something', 'big', '',\n",
    "       'to', 'unsubscribe', 'yourself', 'from', 'this', 'mailing', 'list',\n",
    "       'send', 'an', 'email', 'to', 'emailaddr', '', ''], dtype='<U12')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Remove zero length tokens\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_tokens(tokens):\n",
    "    '''\n",
    "    tokens: ndarray of str\n",
    "    return: ndarray of filtered tokens (str)\n",
    "    '''\n",
    "    original_tokens_len = len(tokens)\n",
    "\n",
    "    # YOUR_CODE. Keep only tokens that length > 0  \n",
    "    # START_CODE\n",
    "    tokens = tokens[np.vectorize(lambda x: len(x) > 0)(tokens)]\n",
    "    # END_CODE\n",
    "\n",
    "    print('Original len = {}\\nRemaining len = {}'.format(original_tokens_len, len(tokens)))    \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len = 69\n",
      "Remaining len = 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a',\n",
       "       'web', 'portal', 'well', 'it', 'depends', 'on', 'how', 'many',\n",
       "       'visitors', 'youre', 'expecting', 'this', 'can', 'be', 'anywhere',\n",
       "       'from', 'less', 'than', 'number', 'bucks', 'a', 'month', 'to', 'a',\n",
       "       'couple', 'of', 'dollarnumber', 'you', 'should', 'checkout',\n",
       "       'httpaddr', 'or', 'perhaps', 'amazon', 'ecnumber', 'if', 'youre',\n",
       "       'running', 'something', 'big', 'to', 'unsubscribe', 'yourself',\n",
       "       'from', 'this', 'mailing', 'list', 'send', 'an', 'email', 'to',\n",
       "       'emailaddr'], dtype='<U12')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = filter_short_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`Original len= 69\n",
    "Remaining len= 61\n",
    "array(['anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depends', 'on', 'how', 'many',\n",
    "       'visitors', 'youre', 'expecting', 'this', 'can', 'be', 'anywhere',\n",
    "       'from', 'less', 'than', 'number', 'bucks', 'a', 'month', 'to', 'a',\n",
    "       'couple', 'of', 'dollarnumber', 'you', 'should', 'checkout',\n",
    "       'httpaddr', 'or', 'perhaps', 'amazon', 'ecnumber', 'if', 'youre',\n",
    "       'running', 'something', 'big', 'to', 'unsubscribe', 'yourself',\n",
    "       'from', 'this', 'mailing', 'list', 'send', 'an', 'email', 'to',\n",
    "       'emailaddr'], dtype='<U12')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Stem tokens\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of stemmed tokens e.g. array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani']...\n",
    "    '''\n",
    "    # YOUR_CODE. replace the tokens by stemmed form. You may need PorterStemmer.stem() \n",
    "    # START_CODE\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = np.vectorize(stemmer.stem)(tokens)\n",
    "    # END_CODE\n",
    "   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
       "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani',\n",
       "       'visitor', 'your', 'expect', 'thi', 'can', 'be', 'anywher', 'from',\n",
       "       'less', 'than', 'number', 'buck', 'a', 'month', 'to', 'a', 'coupl',\n",
       "       'of', 'dollarnumb', 'you', 'should', 'checkout', 'httpaddr', 'or',\n",
       "       'perhap', 'amazon', 'ecnumb', 'if', 'your', 'run', 'someth', 'big',\n",
       "       'to', 'unsubscrib', 'yourself', 'from', 'thi', 'mail', 'list',\n",
       "       'send', 'an', 'email', 'to', 'emailaddr'], dtype='<U10')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = stem_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani',\n",
    "       'visitor', 'your', 'expect', 'thi', 'can', 'be', 'anywher', 'from',\n",
    "       'less', 'than', 'number', 'buck', 'a', 'month', 'to', 'a', 'coupl',\n",
    "       'of', 'dollarnumb', 'you', 'should', 'checkout', 'httpaddr', 'or',\n",
    "       'perhap', 'amazon', 'ecnumb', 'if', 'your', 'run', 'someth', 'big',\n",
    "       'to', 'unsubscrib', 'yourself', 'from', 'thi', 'mail', 'list',\n",
    "       'send', 'an', 'email', 'to', 'emailaddr'], dtype='<U10')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Vocabulary\n",
    "</font>\n",
    "\n",
    "Ususally vocabulary is built from the top most frequent tokens within all available training set.\n",
    "For this task it is already prepared and provided in the `vocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab) = 1899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocabulary(filename):\n",
    "    '''\n",
    "    fn: str - full path to file \n",
    "    return: ndarray of str e.g. array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)\n",
    "    '''\n",
    "    vocab_list = pd.read_table(filename, header=None)\n",
    "    vocab = np.array(vocab_list)[:, 1]  # First columns is index, select only words column  \n",
    "    print('len(vocab) = {}'.format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "file = os.path.join(path, 'vocab.txt')\n",
    "vocab = get_vocabulary(file)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Feature reresentation\n",
    "</font>\n",
    "\n",
    "Every word in vocabulary is the feature of the mail - 1 is the word in in the mail and 0 otherwise. \n",
    "Thus every mail can be represented as the binary array of fixed length - number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_features(tokens, vocab):\n",
    "    '''\n",
    "    tokens: ndarray of str\n",
    "    tokens: ndarray of str\n",
    "    return: ndarray of binary values 1 if word from vocabulary is in mail 0 otherwise\n",
    "    '''\n",
    "    # YOUR_CODE. Compute the array with 1/0 corresponding to is word from vocabulary in mail \n",
    "    # START_CODE\n",
    "    tokens_represented = np.vectorize(lambda x: 1 if x in tokens else 0)(vocab)\n",
    "    # END_CODE\n",
    "\n",
    "    print('{} word(s) from vocab are in the tokens.'.format(np.sum(tokens_represented)))\n",
    "    return tokens_represented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 word(s) from vocab are in the tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_represented = represent_features(tokens, vocab)\n",
    "tokens_represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`44 word(s) from vocab are in the tokens.\n",
    "array([0, 0, 0, ..., 0, 0, 0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Composing all steps of preprocessing \n",
    "</font>\n",
    "\n",
    "Every word in vocabulary is the feature of the mail - 1 is the word in in the mail and 0 otherwise. \n",
    "Thus every mail can be represented as the binary array of fixed length - number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content, vocab):\n",
    "    '''\n",
    "    content: str - body of mail \n",
    "    vocab: ndarray of str - list of considered words \n",
    "    '''\n",
    "    # YOUR_CODE. Compute the array with 1/0 corresponding to is word from vocabulary in mail \n",
    "    # START_CODE \n",
    "\n",
    "    # Tokenize content    \n",
    "    tokens  = word_tokenize(content)\n",
    "    \n",
    "    # Make lower case\n",
    "    tokens = lower_case(tokens)\n",
    "\n",
    "    # Normalize tokens\n",
    "    tokens = normalize_tokens(tokens)\n",
    "\n",
    "    # Remove zero words\n",
    "    tokens = filter_short_tokens(tokens)\n",
    "    \n",
    "    # Stem words\n",
    "    tokens = stem_tokens(tokens)\n",
    "    \n",
    "    # Convert to binary array of features\n",
    "    tokens_represented = represent_features(tokens, vocab)\n",
    "    # END_CODE     \n",
    "    \n",
    "    return tokens_represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len = 69\n",
      "Remaining len = 61\n",
      "44 word(s) from vocab are in the tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(content, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`Original len= 69\n",
    "Remaining len= 61\n",
    "44 word(s) from vocab are in the tokens.\n",
    "array([0, 0, 0, ..., 0, 0, 0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Training and test sets\n",
    "</font>\n",
    "\n",
    "All mails for training are preprocessed the same way as performed for sample above. \n",
    "The training set (array of feature representation) is provided in the `spamTrain.mat`\n",
    "The corresponding test set is provided in the `spamTest.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (4000, 1899)\n",
      "y_train.shape = (4000,)\n",
      "X_test.shape = (1000, 1899)\n",
      "y_test.shape = (1000,)\n",
      "Sample with index = 0: \n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "file = os.path.join(path , 'spamTrain.mat')\n",
    "mat = loadmat(file)\n",
    "X_train = mat['X']\n",
    "y_train = mat['y'].ravel()\n",
    "\n",
    "print('X_train.shape =', X_train.shape)\n",
    "print('y_train.shape =', y_train.shape)\n",
    "\n",
    "# Load testing data\n",
    "file = os.path.join(path, 'spamTest.mat')\n",
    "mat = loadmat(file)\n",
    "X_test = mat['Xtest']\n",
    "y_test = mat['ytest'].ravel() \n",
    "\n",
    "print('X_test.shape =', X_test.shape)\n",
    "print('y_test.shape =', y_test.shape)\n",
    "\n",
    "index = 0\n",
    "print('Sample with index = {}: \\n{}'.format(index, X_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Training  the model\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train = 0.99975\n",
      "Score test = 0.992\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(C=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Score train = {}'.format(clf.score(X_train, y_train)))\n",
    "print('Score test = {}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Determining most spam contributors\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_CODE. Compute top 20 largest coefficients and return the corresponding 20 words from vocabulary\n",
    "# START_CODE\n",
    "\n",
    "# Get coefficients of SVM classifier\n",
    "coefs = np.squeeze(clf.coef_)\n",
    "\n",
    "# Sort coefficients in descending order\n",
    "top_spam = np.flip(np.argsort(coefs))\n",
    "\n",
    "top_spam_contributors = vocab[top_spam[:20]]\n",
    "# END_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our' 'remov' 'click' 'basenumb' 'guarante' 'visit' 'bodi' 'will'\n",
      " 'numberb' 'price' 'dollar' 'nbsp' 'below' 'lo' 'most' 'send' 'dollarnumb'\n",
      " 'credit' 'wi' 'hour']\n"
     ]
    }
   ],
   "source": [
    "print(top_spam_contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['our' 'remov' 'click' 'basenumb' 'guarante' 'visit' 'bodi' 'will'\n",
    " 'numberb' 'price' 'dollar' 'nbsp' 'below' 'lo' 'most' 'send' 'dollarnumb'\n",
    " 'credit' 'wi' 'hour']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Use model for prediction \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len = 69\n",
      "Remaining len = 61\n",
      "44 word(s) from vocab are in the tokens.\n",
      "emailSample1.txt is Not Spam\n",
      "\n",
      "Original len = 247\n",
      "Remaining len = 222\n",
      "122 word(s) from vocab are in the tokens.\n",
      "emailSample2.txt is Not Spam\n",
      "\n",
      "Original len = 141\n",
      "Remaining len = 97\n",
      "46 word(s) from vocab are in the tokens.\n",
      "spamSample1.txt is Spam\n",
      "\n",
      "Original len = 39\n",
      "Remaining len = 31\n",
      "18 word(s) from vocab are in the tokens.\n",
      "spamSample2.txt is Spam\n",
      "\n",
      "Latter sample:\n",
      "==================================================\n",
      "Best Buy Viagra Generic Online\n",
      "\n",
      "Viagra 100mg x 60 Pills $125, Free Pills & Reorder Discount, Top Selling 100% Quality & Satisfaction guaranteed!\n",
      "\n",
      "We accept VISA, Master & E-Check Payments, 90000+ Satisfied Customers!\n",
      "http://medphysitcstech.ru\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for sample_file in ['emailSample1.txt', 'emailSample2.txt', 'spamSample1.txt', 'spamSample2.txt']:\n",
    "    file = os.path.join(path, sample_file)    \n",
    "    content = get_sample(file)\n",
    "    \n",
    "    # YOUR_CODE.  Preprocess the sample and get prediction 0 or 1 (1 is spam)\n",
    "    # START_CODE\n",
    "    prediction = clf.predict(preprocess(content, vocab).reshape(1, -1))[0]\n",
    "    # END_CODE\n",
    "    \n",
    "    print('{} is {}\\n'.format(sample_file, ('Not Spam', 'Spam')[prediction]))\n",
    "\n",
    "print('Latter sample:\\n{1}\\n{0}\\n{1}'.format(content, '='*50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`Original len= 69\n",
    "Remaining len= 61\n",
    "44 word(s) from vocab are in the tokens.\n",
    "emailSample1.txt is Not Spam`\n",
    "\n",
    "`Original len= 247\n",
    "Remaining len= 222\n",
    "122 word(s) from vocab are in the tokens.\n",
    "emailSample2.txt is Not Spam`\n",
    "\n",
    "`Original len= 141\n",
    "Remaining len= 97\n",
    "46 word(s) from vocab are in the tokens.\n",
    "spamSample1.txt is Spam`\n",
    "\n",
    "`Original len= 39\n",
    "Remaining len= 31\n",
    "18 word(s) from vocab are in the tokens.\n",
    "spamSample2.txt is Spam`\n",
    "\n",
    "`Latter sample:`\n",
    "\n",
    "`==================================================`\n",
    "\n",
    "`Best Buy Viagra Generic Online`\n",
    "\n",
    "`Viagra 100mg x 60 Pills $125, Free Pills & Reorder Discount, Top Selling 100\\% Quality & Satisfaction guaranteed!`\n",
    "\n",
    "`We accept VISA, Master & E-Check Payments, 90000+ Satisfied Customers!\n",
    "http://medphysitcstech.ru`\n",
    "\n",
    "\n",
    "`==================================================`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "# Congratulation!\n",
    "</font>\n",
    "\n",
    "Now you know how to build Spam/Non-Spam classifier\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
