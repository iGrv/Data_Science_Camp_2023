{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home task: Topic Modeling \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load data \n",
    "\n",
    "</font>\n",
    "\n",
    "[voted-kaggle-dataset](https://www.kaggle.com/canggih/voted-kaggle-dataset/version/2#voted-kaggle-dataset.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TEXT_FILE = 'voted-kaggle-dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of texts_df = 2,150\n",
      "\n",
      "Example of 11th document:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file. k'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read text data into a DataFrame\n",
    "texts_df = pd.read_csv(TEXT_FILE)\n",
    "\n",
    "print('Length of texts_df = {:,}\\n'.format(len(texts_df)))\n",
    "print('Example of 11th document:')\n",
    "texts_df.loc[10, 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cleared text data = 2,145\n"
     ]
    }
   ],
   "source": [
    "# Remove the NA values from documents\n",
    "texts = texts_df[~texts_df['Description'].isna()]['Description']\n",
    "\n",
    "print('Length of cleared text data = {:,}'.format(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLDOUT = 3\n",
    "\n",
    "# Select 3 random documents for further topic modeling\n",
    "holdout = texts.sample(HOLDOUT, random_state=0)\n",
    "texts.drop(holdout.index)\n",
    "holdout = holdout.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Transform documents into a sparse matrix of words counts (select words having 3 or more characters and exclude stop words)\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=20, max_df=200, token_pattern=r'\\b\\w{3,}\\b')\n",
    "texts_vectorized = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of features = 1,692\n",
      "\n",
      "First 50 features:\n",
      "['100' '1000' '1995' '1st' '200' '2000' '2001' '2003' '2004' '2005' '2006'\n",
      " '2007' '2008' '2009' '2010' '2011' '2012' '2013' '2014' '2018' '2nd'\n",
      " '300' '400' '500' '600' 'ability' 'able' 'abs' 'abstract' 'academic'\n",
      " 'access' 'accessed' 'accessible' 'accompanying' 'according' 'account'\n",
      " 'accounts' 'accuracy' 'accurate' 'accurately' 'achieve' 'achieved'\n",
      " 'acknowledge' 'acknowledgement' 'acknowledgments' 'acquired' 'act'\n",
      " 'action' 'active' 'activities']\n"
     ]
    }
   ],
   "source": [
    "# Review the features of tha matrix\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "print('Length of features = {:,}\\n'.format(len(features)))\n",
    "print('First 50 features:')\n",
    "print(features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of corpus entries:\n",
      "1: [(1553, 4), (385, 1), (210, 1), (1379, 1), (17, 1), (539, 1), (1034, 1), (401, 1), (699, 1), (1133, 1), (255, 3), (35, 1), (764, 1), (1612, 1), (1304, 1), (1580, 1), (796, 1), (1191, 1), (585, 3), (147, 1), (311, 1), (1032, 1), (584, 4), (1359, 1), (1552, 3), (364, 1), (1300, 1), (1611, 1), (1504, 1), (1609, 1), (213, 1), (1224, 1), (37, 2), (114, 1), (924, 1), (257, 2), (679, 1), (166, 1), (963, 2), (432, 1), (393, 1), (1081, 1), (1185, 1), (1260, 1), (1543, 1), (249, 1), (1164, 1), (315, 1), (777, 1), (728, 1)]\n",
      "\n",
      "2: [(210, 1), (539, 1), (255, 1), (585, 1), (213, 1), (1164, 1), (921, 4), (1116, 7), (367, 1), (842, 1), (1356, 1), (12, 1), (1513, 4), (134, 5), (1418, 2), (1435, 3), (1623, 1), (642, 5), (1381, 2), (1650, 1), (1592, 1), (1512, 4), (865, 1), (351, 1), (430, 1), (920, 1), (543, 3), (662, 1), (1573, 1), (390, 1), (1498, 1), (339, 1), (490, 1), (1604, 1), (1648, 1), (274, 1), (1172, 1), (860, 1), (489, 1), (288, 1), (612, 2), (96, 1), (1347, 1), (1578, 1), (279, 1), (643, 3), (1189, 1), (889, 3), (1019, 2), (615, 2), (1419, 1), (197, 1), (739, 1), (966, 1), (1021, 1), (1610, 1), (26, 2), (603, 1), (380, 2), (78, 1), (736, 1), (785, 1), (844, 2), (122, 1), (1426, 1), (1638, 1), (735, 1), (30, 2), (734, 1), (1017, 1), (781, 1), (1682, 1), (1352, 2), (1202, 1), (99, 1), (232, 1), (428, 1), (283, 1), (552, 1), (1351, 1), (1115, 1), (1591, 1), (562, 1), (636, 1), (894, 1), (55, 1), (768, 2), (1060, 1), (1066, 1), (814, 1), (1566, 1), (721, 1), (1055, 1), (256, 1), (703, 3), (1664, 3), (146, 1), (1312, 1), (41, 1), (582, 1), (696, 1), (1212, 1), (1434, 1), (670, 1), (820, 1), (152, 1), (1144, 1), (1324, 1), (983, 1), (259, 1), (1057, 1), (536, 1), (297, 1), (1146, 1), (561, 1), (1632, 1), (1173, 1), (782, 1)]\n",
      "\n",
      "3: [(699, 1), (147, 1), (393, 1), (865, 1), (339, 1), (288, 1), (96, 4), (966, 3), (1610, 1), (26, 1), (30, 2), (734, 1), (552, 2), (768, 1), (1060, 1), (814, 3), (1566, 1), (820, 1), (1173, 1), (1336, 1), (1484, 2), (981, 3), (1267, 1), (225, 1), (295, 1), (902, 2), (0, 1), (958, 1), (1207, 1), (758, 1), (783, 1), (1220, 1), (676, 1), (1107, 2), (1445, 1), (187, 3), (1532, 1), (1488, 1), (1274, 1), (1621, 3), (1288, 1), (1049, 4), (732, 1), (1399, 2), (1522, 1), (148, 2), (1008, 2), (191, 1), (1041, 1), (862, 2), (887, 1), (667, 2), (173, 2), (813, 2), (638, 2), (547, 1), (472, 1), (648, 1), (788, 1), (386, 4), (810, 2), (873, 1), (98, 2), (982, 3), (442, 1), (14, 1), (661, 2), (169, 1), (371, 1), (525, 1), (526, 1), (38, 1), (875, 1), (1444, 1), (337, 1), (807, 1), (881, 1), (1400, 1), (1622, 1), (1156, 1), (1395, 3), (485, 1), (1377, 1), (1602, 1), (476, 2), (1223, 1), (1323, 1), (391, 1), (1148, 1), (1137, 1), (1126, 1), (1455, 1), (893, 1), (277, 1), (1529, 1), (230, 1), (851, 1), (473, 1), (1203, 1), (998, 1), (1689, 1), (591, 1), (601, 1), (694, 1), (812, 1), (1165, 1), (720, 1), (1408, 1), (826, 1), (1134, 1), (188, 1), (801, 1), (1537, 1), (680, 1), (578, 1), (270, 1), (665, 1), (651, 1), (1175, 1), (1603, 1), (1192, 1), (57, 1), (944, 1)]\n",
      "\n",
      "4: [(1034, 1), (1612, 2), (1304, 1), (1418, 1), (1381, 1), (1592, 1), (1512, 1), (662, 1), (274, 6), (288, 2), (1419, 2), (785, 1), (30, 5), (734, 2), (1017, 1), (1060, 1), (1312, 3), (1267, 1), (0, 1), (1107, 1), (1445, 15), (1488, 1), (1522, 3), (648, 1), (661, 7), (526, 2), (38, 1), (1602, 1), (998, 1), (1603, 1), (1192, 2), (57, 1), (92, 1), (640, 1), (475, 1), (741, 1), (1095, 2), (214, 1), (901, 1), (1292, 2), (1476, 4), (1301, 4), (1586, 3), (1676, 1), (1184, 1), (1194, 2), (553, 1), (809, 1), (19, 1), (1583, 1), (130, 1), (884, 1), (1056, 1), (937, 3), (119, 2), (233, 1), (1086, 3), (482, 1), (1561, 1), (1076, 3), (1255, 1), (375, 1), (413, 1), (51, 1), (613, 1), (1013, 3), (1123, 1), (493, 1), (1410, 1), (952, 1), (1338, 1), (420, 4), (1362, 3), (1341, 1), (1517, 1), (1038, 1), (1183, 1), (16, 1), (409, 1), (338, 1), (1286, 1), (1040, 2), (1626, 1), (1584, 3), (672, 2), (1195, 2), (1279, 1), (1101, 1), (228, 1), (73, 3), (321, 1), (1360, 1), (1247, 2), (1174, 1), (923, 2), (1149, 1), (1660, 3), (1646, 3), (1150, 3), (221, 1), (1601, 9), (755, 2), (756, 1), (908, 2), (1439, 5), (1284, 1), (1050, 1), (1281, 1), (1598, 1), (1313, 3), (1354, 1), (1624, 1), (340, 2), (1251, 1), (1131, 1), (69, 1), (43, 1), (250, 1), (610, 1), (1307, 1), (32, 1), (479, 1), (1097, 3), (943, 2), (56, 2), (462, 1), (1200, 1), (892, 2), (107, 2), (849, 1), (503, 1), (239, 1), (577, 1), (36, 1), (1277, 1), (876, 1), (306, 1), (693, 1), (398, 1), (532, 1), (1305, 1), (1306, 1), (1427, 1), (127, 1), (740, 1), (1562, 1), (389, 1), (468, 1), (542, 2), (973, 1), (1273, 2), (59, 1), (467, 1), (1551, 3), (429, 1), (821, 1), (1541, 1), (163, 1), (1169, 1), (1075, 1), (845, 1), (153, 1), (650, 1), (1453, 1), (548, 1), (95, 1), (633, 1), (123, 1), (336, 1), (496, 1)]\n",
      "\n",
      "5: [(1553, 2), (1034, 1), (1552, 2), (1504, 1), (963, 1), (1592, 1), (96, 2), (966, 1), (636, 1), (768, 1), (696, 1), (1324, 1), (782, 1), (1267, 1), (1107, 1), (525, 1), (337, 1), (1156, 1), (1095, 1), (19, 1), (1341, 1), (16, 1), (340, 1), (462, 1), (503, 1), (306, 1), (163, 1), (822, 1), (13, 1), (941, 1), (447, 1), (550, 4), (1241, 1), (999, 1), (1240, 2), (223, 1), (392, 2), (917, 2), (1546, 2), (599, 1), (1417, 1), (609, 1), (700, 1), (960, 2), (686, 1), (799, 2), (964, 2), (896, 1), (266, 1), (1634, 2), (1653, 1), (1158, 1), (50, 1), (1535, 1), (441, 2), (551, 1), (1515, 1), (531, 1), (1278, 1), (646, 1), (357, 1), (25, 1), (1314, 1), (1615, 1), (905, 1), (446, 1), (1350, 1), (354, 1), (1526, 1), (1389, 1), (1473, 1), (1369, 1), (1643, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "# Build a corpus based on the matrix\n",
    "corpus = Sparse2Corpus(texts_vectorized, documents_columns=False)\n",
    "\n",
    "print('Example of corpus entries:')\n",
    "for i, doc in enumerate(corpus):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(f'{i + 1}:', doc, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of 10 random id2word_map entries:\n",
      "1175: product\n",
      "444: difference\n",
      "354: core\n",
      "1502: tags\n",
      "587: federal\n",
      "908: management\n",
      "1200: purpose\n",
      "1056: outcomes\n",
      "591: field\n",
      "995: near\n"
     ]
    }
   ],
   "source": [
    "# Build a vocabulary mapping the word ids to the words\n",
    "id2word_map = {i: word for word, i in vectorizer.vocabulary_.items()}\n",
    "\n",
    "import random\n",
    "ids = random.sample(list(id2word_map), 10)\n",
    "\n",
    "print('Example of 10 random id2word_map entries:')\n",
    "for id in ids:\n",
    "    print(f'{id}: {id2word_map[id]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Create a LDA model with 5 topics using the corpus and the vocabulary\n",
    "model = LdaModel(corpus, num_topics=5, id2word=id2word_map, passes=25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant words for topic 0:\n",
      "0.011*\"column\" + 0.010*\"class\" + 0.010*\"instances\" + 0.009*\"images\" + 0.009*\"activity\" + 0.008*\"cell\" + 0.007*\"features\" + 0.007*\"recognition\" + 0.007*\"image\" + 0.007*\"group\" + 0.006*\"labels\" + 0.006*\"human\" + 0.006*\"body\" + 0.006*\"values\" + 0.006*\"results\" + 0.005*\"solar\" + 0.005*\"size\" + 0.005*\"mean\" + 0.005*\"paper\" + 0.005*\"attribute\"\n",
      "\n",
      "Significant words for topic 1:\n",
      "0.012*\"game\" + 0.012*\"player\" + 0.011*\"team\" + 0.011*\"price\" + 0.009*\"news\" + 0.008*\"company\" + 0.008*\"players\" + 0.007*\"games\" + 0.006*\"score\" + 0.006*\"market\" + 0.006*\"non\" + 0.006*\"integer\" + 0.006*\"reviews\" + 0.006*\"music\" + 0.006*\"match\" + 0.005*\"companies\" + 0.005*\"percentage\" + 0.005*\"python\" + 0.005*\"season\" + 0.005*\"just\"\n",
      "\n",
      "Significant words for topic 2:\n",
      "0.006*\"department\" + 0.006*\"age\" + 0.006*\"health\" + 0.005*\"gov\" + 0.005*\"united\" + 0.005*\"variables\" + 0.005*\"census\" + 0.005*\"survey\" + 0.005*\"country\" + 0.005*\"location\" + 0.005*\"row\" + 0.004*\"education\" + 0.004*\"crime\" + 0.004*\"population\" + 0.004*\"non\" + 0.004*\"statistics\" + 0.003*\"status\" + 0.003*\"records\" + 0.003*\"period\" + 0.003*\"rate\"\n",
      "\n",
      "Significant words for topic 3:\n",
      "0.299*\"university\" + 0.038*\"college\" + 0.016*\"california\" + 0.014*\"tweet\" + 0.012*\"institute\" + 0.010*\"tweets\" + 0.010*\"times\" + 0.010*\"community\" + 0.009*\"twitter\" + 0.009*\"north\" + 0.008*\"technology\" + 0.008*\"science\" + 0.007*\"san\" + 0.006*\"wouldn\" + 0.006*\"user\" + 0.006*\"want\" + 0.006*\"lower\" + 0.006*\"past\" + 0.006*\"vehicle\" + 0.006*\"thanks\"\n",
      "\n",
      "Significant words for topic 4:\n",
      "0.023*\"trained\" + 0.016*\"words\" + 0.015*\"pre\" + 0.015*\"features\" + 0.015*\"language\" + 0.012*\"word\" + 0.009*\"corpus\" + 0.008*\"languages\" + 0.008*\"large\" + 0.008*\"train\" + 0.008*\"english\" + 0.007*\"accuracy\" + 0.007*\"software\" + 0.007*\"deep\" + 0.006*\"learned\" + 0.006*\"contain\" + 0.006*\"depth\" + 0.006*\"image\" + 0.006*\"networks\" + 0.006*\"tags\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the top 20 significant words for each topic\n",
    "for i, words in model.print_topics(num_topics=-1, num_words=20):\n",
    "    print(f'Significant words for topic {i}:')\n",
    "    print(words, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic naming\n",
    "\n",
    "Now we can name the topics using the most significant words. Considering these words:\n",
    "\n",
    "- let name of topic 0 is `Science/Technology`\n",
    "- let name of topic 1 is `Games/Intertainment`\n",
    "- let name of topic 2 is `Healthcare/Demographics`\n",
    "- let name of topic 3 is `Education/Social media`\n",
    "- let name of topic 4 is `Machine learning/Linguistics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the topic indexes to the topic names\n",
    "TOPICS = {\n",
    "    0: 'Science/Technology',\n",
    "    1: 'Games/Intertainment',\n",
    "    2: 'Healthcare/Demographics',\n",
    "    3: 'Education/Social media',\n",
    "    4: 'Machine learning/Linguistics'\n",
    "}\n",
    "\n",
    "def get_topics(text, vectorizer, lda_model):\n",
    "    '''\n",
    "    Builds a topic distribution for document `text` using the count\n",
    "    vectorizer `vectorizer` and the LDA algorithm model `lda_model`\n",
    "    :param text: text of document\n",
    "    :type text: str\n",
    "    :param vectorizer: count vectorizer\n",
    "    :type vectorizer: CountVectorizer\n",
    "    :param lda_model: LDA model\n",
    "    :type lda_model: LdaModel\n",
    "    :return: a dictionary mapping the topic names to the probabilities\n",
    "    the document is related to a specific topic\n",
    "    :rtype: dict[str, float]\n",
    "    '''\n",
    "    # Trasform the document text into a sparse matrix of words counts\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    text_corpus = Sparse2Corpus(text_vectorized, documents_columns=False)\n",
    "\n",
    "    # Costruct a topic distribution for the document using the LDA model\n",
    "    topics, = list(lda_model.get_document_topics(text_corpus))\n",
    "    return {TOPICS[t]: p for t, p in topics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling for document samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1, doc_2, doc_3 = holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story\n",
      "This is a set of 13,000 images from the site https://prnt.sc/. It is a site that enables users to easily upload images, either through the web interface, or, most commonly, through the downloadable screen cap tool which enables easy selection and uploading of an area of your screen. As you can see on their homepage, at the point of posting this, they have almost a billion images uploaded. The amount of information in there will be incredible, it’s an information enthusiast dream. Around 2 years ago I discovered this, and I thought it was interesting to mass download these images with a tool I created, but I was manually looking at every single image. As I became more interested in machine learning, I figured experimenting with the 20,000 or so images I had downloaded at the time from the site would be interesting, especially since because of the nature of the site and its ease of access, it gets used for a few very specific purposes which is very useful for image categorisation. Video games are an extremely popular use, pictures of people, animations, screenshots of chats and the most popular; debugging or technical help. Anyone in this field knows people are not particularly conscious of where they put information. I’m sure you can imagine some of the interesting nuggets of info in here just waiting to be found. I was able to find a fair amount just using a retrained Inception CNN and some OCR. I manually categorised just over 5,600 images into 6 categories:\n",
      "Animations\n",
      "Games\n",
      "Objects\n",
      "People\n",
      "Text\n",
      "A very specific kind of animated game character uploaded frequently enough to deserve its own category\n",
      "I was able to achieve around 85% accuracy for categorisation with the rest of the image set (I have 1,000,000 images downloaded from the site total) using just those manually categorised 5,609 images.\n",
      "Image Collection\n",
      "The way an image is assigned its link on their site is what made it easy to scan and scrape images from their site. The URL codes are generated sequentially out of a combination letters and numbers of length 6, i.e. prnt.sc/jkl123 prnt.sc/jkl124 prnt.sc/jkl125 prnt.sc/jkl126 Would represent images uploaded one after another. This is of course very easy to write a script to scrape and collect images from. So far I have collected 1,000,000 images in total, and I am now making as many of them as I could available here, but of course you can imagine how easy it would be to collect massive amounts of images from this site. My images were collected over the last 2 years, although the vast majority were collected within the last 4 months.\n",
      "Copyright\n",
      "The LightShot terms of service, including their stance on copyright: https://app.prntscr.com/en/privacy.html\n",
      "Inspiration\n",
      "This image set is interesting because it represents a balance between a completely random and chaotic set of images and a very structured set of images, due to the fact that there is a great amount of variance image to image, but essentially every image can be categorised into a set of very specific purposes that the site is used for. Because of this, it is good for learning and testing out image processing models.\n",
      "It is essentially an information gatherers gold mine. There are 1 billion screenshots uploaded by everyday users to be collected and processed on this site, all available sequentially. I’m sure you can imagine the kinds of easily accessible (to a machine learning enthusiast) information waiting to be collected on this site.\n"
     ]
    }
   ],
   "source": [
    "print(doc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for doc_1:\n",
      "\n",
      "Science/Technology: 0.3741\n",
      "Games/Intertainment: 0.3479\n",
      "Healthcare/Demographics: 0.2139\n",
      "Machine learning/Linguistics: 0.0627\n"
     ]
    }
   ],
   "source": [
    "# Determine which topic Document 1 is related to\n",
    "topics = get_topics(doc_1, vectorizer, model)\n",
    "\n",
    "print('Topic distribution for doc_1:', end='\\n\\n')\n",
    "for topic in topics:\n",
    "    print(f'{topic}: {topics[topic]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest probabilities have topics `Science/Technology` and `Games/Intertainment`, so Document 1 can be related to both these topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context\n",
      "Safebooru (safebooru.org) is a tag-based image archive maintained by anime enthusiasts. It allows users to post images and add tags, annotation, translations and comments. It's derived from Danbooru, and differs from it in that it disallows explicit content. It's quite popular, and there are more than 2.3 million posts as of January 24, 2018.\n",
      "Content\n",
      "The data was scraped via Safebooru's online API, then converted from XML to CSV (some attributes were discarded during the conversion to make the whole csv a little smaller). There are 1,934,214 rows of the metadata. Contains images uploaded to safebooru.org in the time range of 2010-01-29 through 2016-11-20.\n",
      "Acknowledgements\n",
      "Banner image taken from https://safebooru.org/index.php?page=post&s=view&id=1514244\n",
      "Inspiration\n",
      "What tags are highly correlated? Can you predict missing tags? Can you predict the score of an image based on its tags?\n"
     ]
    }
   ],
   "source": [
    "print(doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for doc_2:\n",
      "\n",
      "Science/Technology: 0.1588\n",
      "Games/Intertainment: 0.2370\n",
      "Healthcare/Demographics: 0.2819\n",
      "Machine learning/Linguistics: 0.3181\n"
     ]
    }
   ],
   "source": [
    "# Determine which topic Document 2 is related to\n",
    "topics = get_topics(doc_2, vectorizer, model)\n",
    "\n",
    "print('Topic distribution for doc_2:', end='\\n\\n')\n",
    "for topic in topics:\n",
    "    print(f'{topic}: {topics[topic]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic `Machine learning/Linguistics` has the highest probability value in distribution above. Then we can say that Document 2 most probably is related to topic `Machine learning/Linguistics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context\n",
      "Open Payments is a national disclosure program created by the Affordable Care Act (ACA) and managed by Centers for Medicare & Medicaid Services (CMS). The purpose of the program is to promote transparency into the financial relationships between pharmaceutical and medical device industries, and physicians and teaching hospitals. The financial relationships may include consulting fees, research grants, travel reimbursements, and payments from industry to medical practitioners.\n",
      "Content\n",
      "There are 3 datasets that represent 3 different payment types:\n",
      "General Payments: Payments not made in connection with a research agreement. This dataset contains 65 variables.\n",
      "Research Payments: Payments made in connection with a research agreement. This dataset contains 166 variables.\n",
      "Physician Ownership or Investment Interest: Information about physicians who hold ownership or investment interest in the manufacturer/GPO or who have an immediate family member holding such interest. This dataset contains 29 variables.\n",
      "Deleted/Removed Records: Contains any deleted/removed records.\n",
      "A comprehensive methodology overview and data dictionary for each dataset can be found here.\n",
      "Acknowledgements\n",
      "The original datasets can be found here.\n",
      "Inspiration\n",
      "Using the General Payments dataset, can you determine any trends in the total amount of payment to hospitals and physicians across the medical specialties or by the form/nature of the payments?\n",
      "According to the Research Payments dataset, which area(s) of research or the type of drug/medical device receive the most amount of payment?\n"
     ]
    }
   ],
   "source": [
    "print(doc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for doc_3:\n",
      "\n",
      "Science/Technology: 0.1549\n",
      "Games/Intertainment: 0.0671\n",
      "Healthcare/Demographics: 0.7290\n",
      "Education/Social media: 0.0453\n"
     ]
    }
   ],
   "source": [
    "# Determine which topic Document 3 is related to\n",
    "topics = get_topics(doc_3, vectorizer, model)\n",
    "\n",
    "print('Topic distribution for doc_3:', end='\\n\\n')\n",
    "for topic in topics:\n",
    "    print(f'{topic}: {topics[topic]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 3 is related to topic `Healthcare/Demographics`, which have a much higher probability in the distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
