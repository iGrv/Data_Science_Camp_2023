{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Task: diabetes dataset\n",
    "\n",
    "Use diabetes dataset (`sklearn.datasets.load_diabetes`) and apply\n",
    " - Ridge \n",
    " - Lasso\n",
    " - Polynomial\n",
    " - Normal Equation (optionally)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "### Import necessary packages\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "### Load diabetes dataset\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (331, 10)\n",
      "y_train.shape = (331,)\n",
      "X_train[:5] = \n",
      "[[-0.06363517 -0.04464164 -0.03315126 -0.03321323  0.00118295  0.02405115\n",
      "  -0.02499266 -0.00259226 -0.02251653 -0.05906719]\n",
      " [ 0.01264814 -0.04464164 -0.02560657 -0.04009893 -0.03046397 -0.04515466\n",
      "   0.0780932  -0.0763945  -0.07213275  0.01134862]\n",
      " [ 0.03807591  0.05068012  0.00888341  0.04252949 -0.04284755 -0.02104223\n",
      "  -0.03971921 -0.00259226 -0.01811369  0.00720652]\n",
      " [-0.07816532  0.05068012  0.07786339  0.05285804  0.07823631  0.0644473\n",
      "   0.02655027 -0.00259226  0.04067283 -0.00936191]\n",
      " [-0.07453279 -0.04464164 -0.0105172  -0.00567042 -0.06623874 -0.0570543\n",
      "  -0.00290283 -0.03949338 -0.04257085 -0.0010777 ]]\n",
      "y_train[:5] = \n",
      "[214.  98. 127. 233. 168.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the diabets dataset and split data into train and test data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)\n",
    "print('X_train.shape =', X_train.shape)\n",
    "print('y_train.shape =', y_train.shape)\n",
    "print('X_train[:5] = \\n{}'.format(X_train[:5]))\n",
    "print('y_train[:5] = \\n{}'.format(y_train[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "### Ridge\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression:\n",
      "R2 train score = 0.5072634835482679\n",
      "R2 test score = 0.5260667062916086\n",
      "b = 149.98791540785498, \n",
      "w = [ -0.90572311 -11.39061084  26.93579377  11.85086621 -17.84354295\n",
      "   8.53902963  -3.14662357   6.51696437  28.8868917    3.77727244]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge().fit(X_train_scaled, y_train)\n",
    "print('Ridge regression:')\n",
    "print('R2 train score =', ridge_reg.score(X_train_scaled, y_train))\n",
    "print('R2 test score =', ridge_reg.score(X_test_scaled, y_test))\n",
    "print('b = {}, \\nw = {}'.format(ridge_reg.intercept_, ridge_reg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "### Lasso\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression:\n",
      "R2 train score = 0.5045236099772007\n",
      "R2 test score = 0.5149363101451062\n",
      "b = 149.98791540785498, \n",
      "w = [ -0.          -9.52587293  26.88368498  10.55544422  -2.57364251\n",
      "  -0.         -10.89617641   0.          23.96575213   2.90179342]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso().fit(X_train_scaled, y_train)\n",
    "print('Lasso regression:')\n",
    "print('R2 train score =', lasso_reg.score(X_train_scaled, y_train))\n",
    "print('R2 test score =', lasso_reg.score(X_test_scaled, y_test))\n",
    "print('b = {}, \\nw = {}'.format(lasso_reg.intercept_, lasso_reg.coef_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "### Polynomial + Linear regression\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (331, 10)\n",
      "X_train_poly.shape = (331, 65)\n",
      "\n",
      "Polynomial + Linear regression:\n",
      "R2 train score = 0.6207810962295992\n",
      "R2 test score = 0.3472243986719029\n",
      "b = 55.745642090206076, \n",
      "w = [ 1.06137498e+02 -2.77244219e+02  5.11354358e+02  2.51478306e+02\n",
      " -1.82518302e+04  1.59323845e+04  6.66445690e+03  1.74014774e+02\n",
      "  6.57536398e+03  9.66610282e+01  2.78325334e+03  3.85281468e+03\n",
      " -1.53395915e+02  9.33380694e+02  7.84255464e+03 -1.10762461e+04\n",
      " -1.11174456e+03  2.01277652e+03  1.35040875e+03 -1.10327017e+03\n",
      " -1.67413430e+00  2.29828166e+03  2.55277891e+02 -6.62033960e+02\n",
      "  1.81130613e+03  1.37538779e+02 -6.93403727e+03  1.68439720e+03\n",
      "  1.60179356e+03  1.15224299e+03  3.13930733e+03 -8.23706391e+02\n",
      "  6.06446052e+02  9.05587243e+02 -1.25957240e+03  3.92326702e+02\n",
      "  7.84474860e+02 -3.72762355e+02  1.50641940e+04 -1.23251806e+04\n",
      " -3.94541792e+03  3.05725415e+03 -5.21151753e+03 -2.22762962e+03\n",
      "  8.83280542e+04 -1.14624080e+05 -7.24321258e+04 -3.63921143e+04\n",
      " -2.64089121e+04 -4.87133850e+03  3.72219511e+04  4.48634626e+04\n",
      "  2.00114668e+04  1.20913439e+04  9.24913877e+02  1.31372359e+04\n",
      "  9.59115876e+03  1.12090903e+04  1.07482695e+04  3.89815589e+03\n",
      "  9.14000032e+03  8.77867792e+03  2.96364287e+04  3.13815886e+03\n",
      "  1.70557446e+03]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "print('X_train.shape =', X_train.shape)\n",
    "print('X_train_poly.shape =', X_train_poly.shape, end='\\n\\n')\n",
    "\n",
    "poly_lin_reg = LinearRegression().fit(X_train_poly, y_train)\n",
    "print('Polynomial + Linear regression:')\n",
    "print('R2 train score =', poly_lin_reg.score(X_train_poly, y_train))\n",
    "print('R2 test score =', poly_lin_reg.score(X_test_poly, y_test))\n",
    "print('b = {}, \\nw = {}'.format(poly_lin_reg.intercept_, poly_lin_reg.coef_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "### Polynomial + Ridge\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (331, 10)\n",
      "X_train_poly.shape = (331, 65)\n",
      "\n",
      "Polynomial + Ridge regression:\n",
      "R2 train score = 0.42362526245248566\n",
      "R2 test score = 0.4344259300713217\n",
      "b = 148.85618622858442, \n",
      "w = [ 3.13548686e+01 -6.77491832e+01  2.83802154e+02  1.58244638e+02\n",
      "  2.54731660e+01 -1.42663173e+01 -1.30233444e+02  1.16316946e+02\n",
      "  2.39332100e+02  1.08464072e+02  4.38453761e+00  6.06232757e+00\n",
      "  2.10946170e+00  5.77942534e+00 -1.49911751e+00 -3.66518267e+00\n",
      " -2.02206156e+00  1.58808410e+00  6.10479789e+00  2.69543575e+00\n",
      " -4.09102239e-01  5.19660074e+00  4.83995073e+00  2.11093577e-01\n",
      " -2.70505456e+00  2.81483657e+00 -1.21421159e+00  3.22042327e+00\n",
      "  3.67661104e+00  1.60859482e+01  9.27850554e+00 -7.54821067e-01\n",
      " -9.40316125e-01 -3.98034200e+00  4.43693781e+00  4.88816675e+00\n",
      "  6.69811051e+00  7.60613734e+00  2.08426879e+00 -4.79709920e-01\n",
      "  4.02204766e-02  1.69155415e+00  6.14463154e+00  3.19036691e+00\n",
      "  5.87841125e-01 -1.30273961e+00  1.08750029e+00 -1.93062336e+00\n",
      "  1.75755210e+00  3.21770522e+00 -2.72204326e+00  2.43254686e+00\n",
      " -3.21448980e+00 -2.43849169e-01  1.11514019e+00 -2.72666183e+00\n",
      " -4.28949245e-01  2.32673542e-01 -1.74447229e+00  1.65186275e+00\n",
      "  1.49878125e+00  5.72403079e+00  2.68302980e+00  6.38762687e+00\n",
      "  8.79205740e+00]\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "print('X_train.shape =', X_train.shape)\n",
    "print('X_train_poly.shape =', X_train_poly.shape, end='\\n\\n')\n",
    "\n",
    "poly_lin_reg = Ridge().fit(X_train_poly, y_train)\n",
    "print('Polynomial + Ridge regression:')\n",
    "print('R2 train score =', poly_lin_reg.score(X_train_poly, y_train))\n",
    "print('R2 test score =', poly_lin_reg.score(X_test_poly, y_test))\n",
    "print('b = {}, \\nw = {}'.format(poly_lin_reg.intercept_, poly_lin_reg.coef_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "### Polynomial + Lasso\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (331, 10)\n",
      "X_train_poly.shape = (331, 65)\n",
      "\n",
      "Polynomial + Lasso regression:\n",
      "R2 train score = 0.36601908968194896\n",
      "R2 test score = 0.33920924807921515\n",
      "b = 149.48529539341314, \n",
      "w = [  0.          -0.         379.30812187   0.           0.\n",
      "   0.          -0.           0.         317.42349078   0.\n",
      "   0.           0.           0.           0.          -0.\n",
      "  -0.          -0.           0.           0.           0.\n",
      "  -0.           0.           0.           0.          -0.\n",
      "   0.          -0.           0.           0.           0.\n",
      "   0.          -0.          -0.          -0.           0.\n",
      "   0.           0.           0.           0.          -0.\n",
      "   0.           0.           0.           0.           0.\n",
      "  -0.          -0.          -0.           0.           0.\n",
      "  -0.           0.          -0.          -0.           0.\n",
      "  -0.          -0.           0.          -0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "print('X_train.shape =', X_train.shape)\n",
    "print('X_train_poly.shape =', X_train_poly.shape, end='\\n\\n')\n",
    "\n",
    "poly_lin_reg = Lasso().fit(X_train_poly, y_train)\n",
    "print('Polynomial + Lasso regression:')\n",
    "print('R2 train score =', poly_lin_reg.score(X_train_poly, y_train))\n",
    "print('R2 test score =', poly_lin_reg.score(X_test_poly, y_test))\n",
    "print('b = {}, \\nw = {}'.format(poly_lin_reg.intercept_, poly_lin_reg.coef_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "### Normal Equation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving linear regression using normal equation...\n",
      "b = 148.99290898243788, \n",
      "w = [[ -19.6849459  -240.17712443  557.92071086  251.49875073 -500.35528341\n",
      "   275.55002947  -11.62872458  154.0055582   651.15320811   77.51418657]]\n",
      "\n",
      "Predicting using normal equation...\n",
      "R2 train score = 0.5073693366380001\n",
      "R2 test score = 0.5281729599217625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class NormalEquation:\n",
    "\n",
    "    def h(self, b, w, X):\n",
    "        '''\n",
    "        :param b - float or ndarray of shape (m, 1), m - number of samples\n",
    "        :param w - ndarray of shape (1, m), n - number of features\n",
    "        :param X - ndarray of shape (m, n), m - number of samples, n - number of features\n",
    "        '''\n",
    "\n",
    "        h_res = b + X @ w.T\n",
    "        return h_res\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        :param X - ndarray training set of shape (m, n), m - number of samples, n - number of features\n",
    "        :param y - ndarray - 1d array \n",
    "        :return: True in case of successful fit\n",
    "        '''\n",
    "        \n",
    "        m, n = X.shape\n",
    "\n",
    "        # Add a column filled with ones\n",
    "        X_ext = np.hstack((np.ones((m, 1)), X))\n",
    "        assert X_ext.shape == (m, n + 1)\n",
    "\n",
    "        # Use normal equation formula to compute linear regression parameters\n",
    "        params = np.linalg.inv(X_ext.T @ X_ext) @ X_ext.T @ y\n",
    "\n",
    "        # Store parameters for further using\n",
    "        self.intercept_ = params[0]\n",
    "        self.coef_ = params[1:].reshape(1, -1)\n",
    "    \n",
    "    def predict(self, X): \n",
    "        '''\n",
    "        :param X - ndarray of shape (?, n)\n",
    "        :return \n",
    "        '''\n",
    "        return self.h(self.intercept_, self.coef_, X)\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        '''\n",
    "        :param X_test - ndarray testing set or any for prediction of shape (?, n), ? - number of samples, n - number of features\n",
    "        :param y_test - ndarray - 1d array \n",
    "        :return R2 score of y_test and prediction for X_test\n",
    "        '''\n",
    "        z = self.predict(X_test)\n",
    "        return r2_score(y_test, z)\n",
    "\n",
    "\n",
    "print('Solving linear regression using normal equation...')\n",
    "norm_eq = NormalEquation()\n",
    "norm_eq.fit(X_train, y_train)\n",
    "print('b = {}, \\nw = {}'.format(norm_eq.intercept_, norm_eq.coef_), end='\\n\\n')\n",
    "\n",
    "print('Predicting using normal equation...')\n",
    "print('R2 train score =', norm_eq.score(X_train, y_train))\n",
    "print('R2 test score =', norm_eq.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
