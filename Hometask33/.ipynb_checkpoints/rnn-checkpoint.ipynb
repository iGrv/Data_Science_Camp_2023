{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Orf6AsF-iTHo"
   },
   "source": [
    "## Home task: Recurrent neural networks (RNNs)\n",
    "\n",
    "1) Find text to train (any book)<br>\n",
    "2) Build train and validation set<br>\n",
    "3) Train bidirectional language model that predicts the PoS of word being based on its `n_context = 3` neighbours from the left and `n_context = 3` neighbours from the right<br>\n",
    "4) Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq5tGkcxLW9U"
   },
   "source": [
    "---\n",
    "\n",
    "The novel Moby Dick by Herman Melville is used as a text for training a recurrent neural network. [Moby Dick](https://www.gutenberg.org/ebooks/2701) is contained in the Gutenberg corpus available in NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YQ2aT5_iTH4"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glk27Wx9iTH_",
    "outputId": "13796c79-ef82-44ba-d30a-e9698f65170f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DZwZHWmiZhp",
    "outputId": "64979199-57ec-4a98-ccf4-8c722fed3135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moby Dick by Herman Melville 1851]\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\r\n",
      "\r\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
      "\r\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
      "now.  He was ever dusting his old lexicons and grammars, with a queer\r\n",
      "handkerchief, mockingly embellished with all the gay flags of all the\r\n",
      "known nations of the world.  He loved to dust his old grammars; it\r\n",
      "somehow mildly reminded him of his mortality.\r\n",
      "\r\n",
      "\"While you take in hand to school others, and to teac\n"
     ]
    }
   ],
   "source": [
    "# Download the text of Moby Dick\n",
    "moby_dick = gutenberg.raw('melville-moby_dick.txt')\n",
    "print(moby_dick[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-r8AYrSldl9",
    "outputId": "f9bcf8ab-abd6-4f0e-9674-d44cb1fad975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "etymology.\r\n",
      "\r\n",
      "supplied by a late consumptive usher to a grammar school\r\n",
      "\r\n",
      "the pale usher--threadbare in coat, heart, body, and brain; i see him\r\n",
      "now.  he was ever dusting his old lexicons and grammars, with a queer\r\n",
      "handkerchief, mockingly embellished with all the gay flags of all the\r\n",
      "known nations of the world.  he loved to dust his old grammars; it\r\n",
      "somehow mildly reminded him of his mortality.\r\n",
      "\r\n",
      "while you take in hand to school others, and to teach them by what\r\n",
      "name a whale-fish is t\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    Converts the text into lowercase, removes digits and special symbols from it\n",
    "    :param text: text to pre-process\n",
    "    :type text: str\n",
    "    :return: pre-processed text\n",
    "    :rtype: str\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*\\]', '', text)\n",
    "    text = re.sub(r'\\d+', \"\", text)\n",
    "    text = re.sub(r'[\"|()_]', \"\", text)\n",
    "    return text\n",
    "\n",
    "# Pre-process the Moby Dick text\n",
    "moby_dick = preprocess(moby_dick)\n",
    "print(moby_dick[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_l9d6-eXkYFc",
    "outputId": "5f28fe83-f221-401c-a2a8-777d6c4021ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 16948\n",
      "Features:\n",
      "['a' 'aback' 'abaft' 'abandon' 'abandoned' 'abandonedly' 'abandonment'\n",
      " 'abased' 'abasement' 'abashed' 'abate' 'abated' 'abatement' 'abating'\n",
      " 'abbreviate' 'abbreviation' 'abeam' 'abed' 'abednego' 'abel' 'abhorred'\n",
      " 'abhorrence' 'abhorrent' 'abhorring' 'abide' 'abided' 'abiding' 'ability'\n",
      " 'abjectly' 'abjectus' 'able' 'ablutions' 'aboard' 'abode' 'abominable'\n",
      " 'abominate' 'abominated' 'abomination' 'aboriginal' 'aboriginally'\n",
      " 'aboriginalness' 'abortion' 'abortions' 'abound' 'abounded' 'abounding'\n",
      " 'aboundingly' 'about' 'above' 'abraham']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use the count vectorizer to get unique words of the text\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b').fit([moby_dick])\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f'Number of features: {len(vocab)}')\n",
    "print('Features:')\n",
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuVMObWKqves"
   },
   "outputs": [],
   "source": [
    "# Construct dictionaries mapping words to their indexes and vice versa\n",
    "word2index = vectorizer.vocabulary_\n",
    "index2word = {index: word for word, index in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7C28Xapnrxef",
    "outputId": "ba6bfe1b-0355-4e76-bea5-9f327838b95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 218370\n",
      "Number of unique tokens: 16948\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text based on count vectorizer\n",
    "word_tokenize = vectorizer.build_tokenizer()\n",
    "tokens = word_tokenize(moby_dick)\n",
    "n_tokens = len(tokens)\n",
    "print(f'Number of tokens: {n_tokens}')\n",
    "\n",
    "# Get unique tokens\n",
    "unique_tokens = sorted(set(tokens))\n",
    "print(f'Number of unique tokens: {len(unique_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-M2N4-oxwlep",
    "outputId": "0da339a5-e582-40a2-ac55-5b454e1c539f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWPnbvoXwZF-",
    "outputId": "f4729196-8d67-4d97-9e55-7e6630efc07f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 218364\n",
      "First 10 contexts and targets:\n",
      "['etymology', 'supplied', 'by', 'late', 'consumptive', 'usher'] -> [DT]\n",
      "['supplied', 'by', 'a', 'consumptive', 'usher', 'to'] -> [JJ]\n",
      "['by', 'a', 'late', 'usher', 'to', 'a'] -> [NN]\n",
      "['a', 'late', 'consumptive', 'to', 'a', 'grammar'] -> [NN]\n",
      "['late', 'consumptive', 'usher', 'a', 'grammar', 'school'] -> [TO]\n",
      "['consumptive', 'usher', 'to', 'grammar', 'school', 'the'] -> [DT]\n",
      "['usher', 'to', 'a', 'school', 'the', 'pale'] -> [NN]\n",
      "['to', 'a', 'grammar', 'the', 'pale', 'usher'] -> [NN]\n",
      "['a', 'grammar', 'school', 'pale', 'usher', 'threadbare'] -> [DT]\n",
      "['grammar', 'school', 'the', 'usher', 'threadbare', 'in'] -> [NN]\n"
     ]
    }
   ],
   "source": [
    "# Get PoS tags for each token\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# Construct contexts and targets for the dataset\n",
    "n_context = 3\n",
    "contexts = []\n",
    "targets = []\n",
    "for i in range(n_context, n_tokens - n_context):\n",
    "\n",
    "    # Context is 3 words to the left and 3 words to the right of the target word\n",
    "    left_context = [word for word, _ in tagged[i - n_context:i]]\n",
    "    right_context = [word for word, _ in tagged[i + 1:i + n_context + 1]]\n",
    "    contexts.append(left_context + right_context)\n",
    "\n",
    "    # Target is the PoS of the target word\n",
    "    _, target = tagged[i]\n",
    "    targets.append(target)\n",
    "\n",
    "print(f'Number of samples: {len(contexts)}')\n",
    "print('First 10 contexts and targets:')\n",
    "for i in range(10):\n",
    "    print(f'{contexts[i]} -> [{targets[i]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sb8HYgHo7kUG"
   },
   "outputs": [],
   "source": [
    "# Construct dictionaries mapping PoS to their indexes and vice versa\n",
    "pos2index = {pos: index for index, pos in enumerate(set(targets))}\n",
    "index2pos = {index: pos for pos, index in pos2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAIdalHy4VP0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Construct features (contexts) and labels (targets)\n",
    "X = []\n",
    "y = []\n",
    "for context, target in zip(contexts, targets):\n",
    "    X.append([word2index[word] for word in context])\n",
    "    y.append(pos2index[target])\n",
    "\n",
    "# Convert targets to one-hot represatation\n",
    "X = np.array(X)\n",
    "y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MC8HG3CZcx1"
   },
   "outputs": [],
   "source": [
    "n_words = len(word2index)\n",
    "n_pos = len(pos2index)\n",
    "\n",
    "# Define a class for recurrent neural network model\n",
    "class PredictPosModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, n_context):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            n_words, 64, input_length=(2 * n_context)\n",
    "        )\n",
    "        self.bidirectional = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(128, return_sequences=False)\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(n_pos, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.bidirectional(x)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SrU3JywdmY0",
    "outputId": "c98b18f0-5bbf-45c9-c699-82588a131345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1024/1024 [==============================] - 63s 57ms/step - loss: 2.2976 - accuracy: 0.3314\n",
      "Epoch 2/20\n",
      "1024/1024 [==============================] - 58s 57ms/step - loss: 1.7550 - accuracy: 0.4823\n",
      "Epoch 3/20\n",
      "1024/1024 [==============================] - 59s 57ms/step - loss: 1.4966 - accuracy: 0.5539\n",
      "Epoch 4/20\n",
      "1024/1024 [==============================] - 57s 55ms/step - loss: 1.2834 - accuracy: 0.6181\n",
      "Epoch 5/20\n",
      "1024/1024 [==============================] - 59s 58ms/step - loss: 1.1057 - accuracy: 0.6720\n",
      "Epoch 6/20\n",
      "1024/1024 [==============================] - 56s 55ms/step - loss: 0.9592 - accuracy: 0.7151\n",
      "Epoch 7/20\n",
      "1024/1024 [==============================] - 57s 56ms/step - loss: 0.8418 - accuracy: 0.7495\n",
      "Epoch 8/20\n",
      "1024/1024 [==============================] - 58s 57ms/step - loss: 0.7459 - accuracy: 0.7769\n",
      "Epoch 9/20\n",
      "1024/1024 [==============================] - 56s 55ms/step - loss: 0.6652 - accuracy: 0.8008\n",
      "Epoch 10/20\n",
      "1024/1024 [==============================] - 57s 56ms/step - loss: 0.5959 - accuracy: 0.8213\n",
      "Epoch 11/20\n",
      "1024/1024 [==============================] - 56s 55ms/step - loss: 0.5386 - accuracy: 0.8376\n",
      "Epoch 12/20\n",
      "1024/1024 [==============================] - 56s 55ms/step - loss: 0.4837 - accuracy: 0.8537\n",
      "Epoch 13/20\n",
      "1024/1024 [==============================] - 57s 56ms/step - loss: 0.4371 - accuracy: 0.8680\n",
      "Epoch 14/20\n",
      "1024/1024 [==============================] - 56s 55ms/step - loss: 0.3969 - accuracy: 0.8792\n",
      "Epoch 15/20\n",
      "1024/1024 [==============================] - 57s 56ms/step - loss: 0.3585 - accuracy: 0.8913\n",
      "Epoch 16/20\n",
      "1024/1024 [==============================] - 60s 59ms/step - loss: 0.3232 - accuracy: 0.9023\n",
      "Epoch 17/20\n",
      "1024/1024 [==============================] - 62s 60ms/step - loss: 0.2909 - accuracy: 0.9119\n",
      "Epoch 18/20\n",
      "1024/1024 [==============================] - 59s 57ms/step - loss: 0.2614 - accuracy: 0.9210\n",
      "Epoch 19/20\n",
      "1024/1024 [==============================] - 60s 59ms/step - loss: 0.2343 - accuracy: 0.9296\n",
      "Epoch 20/20\n",
      "1024/1024 [==============================] - 62s 60ms/step - loss: 0.2106 - accuracy: 0.9370\n"
     ]
    }
   ],
   "source": [
    "# Create a RNN model and compile it\n",
    "model = PredictPosModel(n_context)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=128);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0Kii0Lzg8SS",
    "outputId": "d3ead441-0751-4e8f-db8a-8506dccb05ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.6058\n",
      "Test accuracy: 0.3940\n"
     ]
    }
   ],
   "source": [
    "# Compute loss and accuracy of the model based on test set\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
